{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from math import floor,ceil\n",
    "from gensim.matutils import kullback_leibler, jaccard, hellinger, cossim, jensen_shannon\n",
    "import os\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from nltk.probability import FreqDist\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to load individual document from a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadBook(folder,filename):\n",
    "    filepath=folder + \"/\" + filename\n",
    "    f=open(filepath)\n",
    "    raw=f.read()\n",
    "    return(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to load all the books from a folder, break them into sentences and remove stopwords from each sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadAndProcess(folder): \n",
    "    docs=list()\n",
    "    for file in os.listdir(folder):\n",
    "        #print(file)\n",
    "        book=loadBook(folder,file)\n",
    "        docs.append(book)\n",
    "    \n",
    "    text=\" \".join(docs)\n",
    "    sents=sent_tokenize(text) #break the document into smaller sentences\n",
    "    \n",
    "    #remove stop words\n",
    "    for i in range(len(sents)):\n",
    "        words=nltk.tokenize.RegexpTokenizer(r'\\w+').tokenize(sents[i])\n",
    "        words=[w.lower() for w in words if w.lower() not in stopwords.words('english')]\n",
    "        words=\" \".join(words)\n",
    "        sents[i]=words\n",
    "        \n",
    "    return(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to load the docs without any pre-processing, i.e. original sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loadOnly(folder):\n",
    "    docs=list()\n",
    "    for file in os.listdir(folder):\n",
    "        #print(file)\n",
    "        book=loadBook(folder,file)\n",
    "        docs.append(book)\n",
    "    \n",
    "    text=\" \".join(docs)\n",
    "    sents=sent_tokenize(text) #break the document into smaller sentences\n",
    "\n",
    "    return(sents)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare a dictionary and corpus for lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepareCorpus(sentences):\n",
    "    text=list()\n",
    "    for i in range(len(sentences)):\n",
    "        words=nltk.word_tokenize(sentences[i])\n",
    "        text.append(words)\n",
    "    dictionary = corpora.Dictionary(text)\n",
    "    corpus = [dictionary.doc2bow(t) for t in text]\n",
    "    return(dictionary,corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions that we will use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateSimilarityMatrix():\n",
    "    similarityMatrix=np.zeros((numOfTopics,numOfTopics))\n",
    "    for i in range(numOfTopics):\n",
    "        ti_dist=sorted(ldamodel.get_topic_terms(i,topn=ldamodel.num_terms))\n",
    "        for j in range(numOfTopics):\n",
    "            tj_dist=sorted(ldamodel.get_topic_terms(j,topn=ldamodel.num_terms))\n",
    "            sim1=1-hellinger(ti_dist,tj_dist)\n",
    "            sim2=1-jensen_shannon(tj_dist,ti_dist)\n",
    "            sim=(sim1+sim2)/2\n",
    "            similarityMatrix[i,j]=sim\n",
    "    return(similarityMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def greaterThanThreshhold(matrix,thresh):\n",
    "    m,n=matrix.shape\n",
    "    greaterThan=False\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            if i==j:\n",
    "                continue\n",
    "            else:\n",
    "                if matrix[i,j]>thresh:\n",
    "                    greaterThan=True\n",
    "    return(greaterThan) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load and run lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "original=loadOnly(\"docs\")\n",
    "#original sentences without any pre processing\n",
    "sents=loadAndProcess(\"docs\") #processed sentences\n",
    "dic,corp=prepareCorpus(sents) #dictionary and corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set number of topics to 2 times the number of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numOfDocs=len(os.listdir(\"docs\"))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numOfTopics=2*numOfDocs         #change as per need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 iteration\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "while(True):\n",
    "    print(i,\"iteration\")\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus=corp, num_topics=numOfTopics, id2word = dic, passes=20)    \n",
    "    sm=generateSimilarityMatrix()\n",
    "    greaterThan=greaterThanThreshhold(sm,0.55)\n",
    "    \n",
    "    if(greaterThan):\n",
    "        numOfTopics=numOfTopics-1\n",
    "    else:\n",
    "        print(numOfTopics)\n",
    "        break\n",
    " \n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a sentences-topic matrix which is stored in a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docsTopicMatrix=np.zeros((len(sents),numOfTopics))\n",
    "for i in range(len(sents)):\n",
    "    topicsList=ldamodel.get_document_topics(corp[i],minimum_probability=0)\n",
    "    for j in range(len(topicsList)):\n",
    "        docsTopicMatrix[i,j]=topicsList[j][1]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each topic we pick out the best (2\\*n/3k). This gives us a reduced set of sentences for each topic. These sentence numbers are stored in a numpy array.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numOfSents=docsTopicMatrix.shape[0]\n",
    "reducedNumOfSents=floor((2*numOfSents)/(3*numOfTopics))\n",
    "reduced_sent_matrix=np.zeros((reducedNumOfSents,numOfTopics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through each column of the docs-Topic matrix and pick out the top 2n/3k sentence numbers for each topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for j in range(numOfTopics):\n",
    "    #for each column, store sentence number and weight pairs in a matrix\n",
    "    temp_dict={}\n",
    "    for i in range(numOfSents):\n",
    "        temp_dict[i]=docsTopicMatrix[i,j]\n",
    "    \n",
    "    #choose the top 2n/3k sentences by finding the max element and deleting it from the dictionary 2n/3k times     \n",
    "    for i in range(reducedNumOfSents):\n",
    "        k=max(temp_dict, key=temp_dict.get)\n",
    "        v=temp_dict.pop(k, None)\n",
    "        reduced_sent_matrix[i,j]=k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  30.,  337.,   99.,  350.,  365.,  270.,    0.,  390.,   95.,\n",
       "         422.,  130.,  127.,  347.,  282.,  101.,   26.,   66.,  228.,\n",
       "         117.,   84.,  133.,  124.,  274.,  225.,  386.,  358.,  423.,\n",
       "         231.,  265.,  233.,   89.,  197.,  346.,  119.,  408.,   21.,\n",
       "         136.,  394.,  148.,  203.,  246.,  316.,  261.],\n",
       "       [ 443.,  177.,   20.,  396.,  290.,  205.,  128.,  260.,   63.,\n",
       "         333.,  399.,  109.,  361.,    2.,  336.,  214.,  204.,  292.,\n",
       "         264.,   36.,  140.,  191.,  150.,  298.,  161.,  404.,  416.,\n",
       "         307.,  421.,  309.,  348.,   87.,  403.,   74.,  339.,  209.,\n",
       "         243.,  210.,  406.,  155.,  279.,   54.,  121.],\n",
       "       [ 344.,  278.,   17.,  362.,   86.,  430.,  248.,  412.,   24.,\n",
       "         212.,  141.,    9.,  257.,  213.,  219.,  284.,   23.,  163.,\n",
       "         413.,   19.,   70.,  398.,  185.,  144.,   18.,  387.,  300.,\n",
       "         166.,   32.,  374.,  349.,  354.,   33.,   96.,  323.,  440.,\n",
       "         444.,  437.,  221.,  241.,  183.,  199.,  162.],\n",
       "       [ 139.,   55.,  352.,  414.,  156.,  436.,   60.,  417.,   76.,\n",
       "         232.,   59.,   10.,  137.,  277.,  289.,  236.,   42.,  369.,\n",
       "         325.,  181.,  388.,  153.,  135.,   61.,  129.,  425.,  227.,\n",
       "         266.,  359.,   65.,   31.,  186.,  321.,  151.,  366.,  230.,\n",
       "         302.,  196.,  294.,  313.,  315.,  340.,   81.],\n",
       "       [ 100.,  375.,  371.,  201.,  401.,  409.,  291.,  158.,  343.,\n",
       "         308.,  218.,   38.,  222.,  283.,  341.,  167.,   43.,  304.,\n",
       "         439.,  363.,  180.,  152.,  102.,  229.,  357.,  407.,    1.,\n",
       "         269.,  415.,  145.,   35.,  138.,  395.,    4.,  372.,  306.,\n",
       "          83.,   28.,   27.,  115.,  273.,  254.,  393.],\n",
       "       [ 318.,  179.,  317.,    9.,  432.,  329.,  131.,  411.,  345.,\n",
       "         172.,  288.,   39.,  295.,  275.,  442.,  384.,  424.,  434.,\n",
       "         147.,  118.,  332.,  184.,  360.,  305.,  176.,  208.,  402.,\n",
       "         103.,  431.,  268.,   67.,  240.,  263.,   57.,    9.,  319.,\n",
       "         376.,  426.,  108.,  256.,   49.,  303.,  149.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_sent_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "doc2vec feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taggeddoc=[]\n",
    "text=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(sents)):\n",
    "    t=nltk.word_tokenize(sents[i])\n",
    "    text.append(t)\n",
    "    td = TaggedDocument(gensim.utils.to_unicode(str.encode(' '.join(t))).split(),tags=[u'Sent_{:d}'.format(i)])\n",
    "    taggeddoc.append(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Doc2Vec(taggeddoc,alpha=0.025, size= 200, min_alpha=0.025, min_count=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now training epoch 0\n",
      "Now training epoch 5\n",
      "Now training epoch 10\n",
      "Now training epoch 15\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    if(epoch%5==0):\n",
    "        print('Now training epoch %s'%epoch)\n",
    "    model.train(taggeddoc,total_examples=model.corpus_count,epochs=model.iter)\n",
    "    model.alpha -= 0.002 \n",
    "    model.min_alpha = model.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features=np.zeros((len(sents),200))\n",
    "for i in range(len(sents)):\n",
    "    features[i]=model.docvecs[u'Sent_{:d}'.format(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_query_sent_doc2vec(sentences):\n",
    "    #get length of query: average number of non stop words per sentence\n",
    "    #most frequent words\n",
    "    temp_text=\" \".join(sentences)\n",
    "    temp_words=nltk.word_tokenize(temp_text)\n",
    "    length=floor((len(temp_words)/len(sentences)))\n",
    "    fdist = FreqDist(temp_words)\n",
    "    importantWords=fdist.most_common(length)\n",
    "    temp=[]\n",
    "    for i in importantWords:\n",
    "        temp.append(i[0])\n",
    "    return(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query=get_query_sent_doc2vec(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sununu',\n",
       " 'bush',\n",
       " 'mr',\n",
       " 'president',\n",
       " 'house',\n",
       " 'staff',\n",
       " 'said',\n",
       " 'white',\n",
       " 'new',\n",
       " 'chief',\n",
       " 'political',\n",
       " 'washington',\n",
       " 'hampshire',\n",
       " 'adams',\n",
       " 'one',\n",
       " 'john']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query_vector=model.infer_vector(query,alpha=0.025,steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0602877448641\n",
      "0.0273545885373\n",
      "0.0590498724867\n",
      "0.0512722804145\n",
      "0.0524075454053\n",
      "0.0435306677154\n",
      "0.0391607759014\n",
      "0.0463311798779\n",
      "0.0470840224827\n",
      "-0.0803105710284\n",
      "0.0709154963809\n",
      "0.0487430510317\n",
      "0.0417586031362\n",
      "0.0504012609099\n",
      "0.0502347227474\n",
      "0.0651553221329\n",
      "0.0828852702038\n",
      "0.0586229969776\n",
      "0.047382763338\n",
      "0.0451092341595\n",
      "0.0589195813205\n",
      "0.0518746466255\n",
      "0.0344467229317\n",
      "0.0431434178049\n",
      "0.0473675764026\n",
      "0.0511344228504\n",
      "0.0437612638737\n",
      "0.0466482748658\n",
      "0.0451318474414\n",
      "0.0484651658786\n",
      "0.0536791538711\n",
      "0.0469064519355\n",
      "0.0485201562877\n",
      "0.0305496596124\n",
      "0.0605561496712\n",
      "0.0460347270379\n",
      "0.0448030689406\n",
      "0.0558040054603\n",
      "-0.0276911293263\n",
      "0.0693175362998\n",
      "0.064419781054\n",
      "0.0550341708228\n",
      "0.0475347529404\n",
      "0.0477032787794\n",
      "0.0530631341174\n",
      "0.0460704069569\n",
      "0.0538938666693\n",
      "0.0497952110688\n",
      "0.0433285084845\n",
      "0.0726313696606\n",
      "0.0375919350224\n",
      "0.0581750665516\n",
      "0.0566075461857\n",
      "0.0507675484794\n",
      "0.0528136699033\n",
      "0.0498606043202\n",
      "0.0573921677367\n",
      "0.044165379311\n",
      "0.0440429157232\n",
      "0.0482566639128\n",
      "0.0532533630337\n",
      "0.0547147534916\n",
      "0.0410220441102\n",
      "0.0474039666062\n",
      "0.0475701863178\n",
      "0.0473790798233\n",
      "0.0530761668949\n",
      "0.0447934569355\n",
      "0.0447606889669\n",
      "0.0497034850578\n",
      "0.0575612006982\n",
      "0.0463161265952\n",
      "0.0368548281147\n",
      "0.00975796983069\n",
      "0.0310534730827\n",
      "0.0254891504917\n",
      "0.0463478593343\n",
      "0.0503425725014\n",
      "0.0567860651227\n",
      "0.0510456475092\n",
      "0.0491422797812\n",
      "0.0458799957171\n",
      "0.0508529789264\n",
      "0.0494010499952\n",
      "0.0494192958625\n",
      "0.0541093194683\n",
      "0.0502519924723\n",
      "0.0332044109157\n",
      "0.0397436597958\n",
      "0.0653882504509\n",
      "0.0471210942536\n",
      "0.0491047057431\n",
      "0.0489109262104\n",
      "0.049176733394\n",
      "0.0648476705413\n",
      "0.0428169752832\n",
      "0.04781996006\n",
      "0.0445821138164\n",
      "0.0427280967804\n",
      "0.0485051322877\n",
      "0.0497281955779\n",
      "0.050656346569\n",
      "0.0497413664986\n",
      "0.0504598444592\n",
      "0.0536695815734\n",
      "0.00809540398622\n",
      "-0.0888554310537\n",
      "0.0519362168429\n",
      "0.0527752186338\n",
      "0.0049183208102\n",
      "0.0442605474491\n",
      "0.0394159154347\n",
      "0.0469811713459\n",
      "0.052930081186\n",
      "0.0480631918179\n",
      "0.0487160616524\n",
      "0.0492334193353\n",
      "0.0485624520316\n",
      "0.0524956202831\n",
      "0.0508203491107\n",
      "0.0453102837689\n",
      "0.049179061361\n",
      "0.0391934358547\n",
      "0.0638113080387\n",
      "0.0426667105136\n",
      "0.00267782130989\n",
      "0.022731318941\n",
      "0.0478502404358\n",
      "0.012025061636\n",
      "0.0476962250719\n",
      "0.040680067307\n",
      "0.0385443052197\n",
      "0.0139173039278\n",
      "0.0340909427317\n",
      "0.0477479034596\n",
      "0.031404070115\n",
      "0.0400307666833\n",
      "0.0443745810455\n",
      "0.0447061607305\n",
      "0.0307558622776\n",
      "0.0218231872945\n",
      "0.0353173208469\n",
      "0.0369313827664\n",
      "0.0415922861586\n",
      "0.0294693368999\n",
      "0.0601397812484\n",
      "0.0559621057062\n",
      "0.0506115285966\n",
      "0.0573638671545\n",
      "0.0496267541564\n",
      "0.0475006247386\n",
      "0.0616397741338\n",
      "0.0444966387838\n",
      "0.0517937436187\n",
      "0.0407697186339\n",
      "0.043729570775\n",
      "0.0563713240147\n",
      "0.0519760775574\n",
      "0.0272776144223\n",
      "0.0322904550206\n",
      "0.0332694937522\n",
      "0.0314290269982\n",
      "0.0347804433456\n",
      "0.0467134278226\n",
      "0.0358575318123\n",
      "0.0374563916292\n",
      "0.0352753387141\n",
      "0.0363324390843\n",
      "-0.0680686868419\n",
      "0.119147351318\n",
      "-0.11247841043\n",
      "0.0511245489374\n",
      "0.050733815488\n",
      "0.0201675008969\n",
      "0.0446563032648\n",
      "0.0491128517447\n",
      "0.0495513036387\n",
      "0.0443598358581\n",
      "0.0485194039775\n",
      "-0.0281184200933\n",
      "0.0447862103858\n",
      "0.0465841548005\n",
      "0.0455130594117\n",
      "0.0618594372709\n",
      "0.0579195851591\n",
      "0.0523043659246\n",
      "0.0509129525985\n",
      "0.0504021428314\n",
      "0.0510178974388\n",
      "0.0534385407122\n",
      "0.0495205663149\n",
      "0.0395088643738\n",
      "0.0494532554243\n",
      "0.0412371171593\n",
      "0.0363160338311\n",
      "0.0500950462585\n",
      "0.0479903055542\n",
      "0.0404843864456\n",
      "0.0546199376918\n",
      "0.0341876556401\n",
      "0.0479085810786\n",
      "0.0486436163462\n",
      "0.047659848752\n",
      "0.0353483391428\n",
      "0.0437934230186\n",
      "0.0499265289538\n",
      "0.0836325716951\n",
      "0.0183771180839\n",
      "0.053962899291\n",
      "0.0702224127401\n",
      "0.05279801348\n",
      "0.0416859043531\n",
      "0.0412236313627\n",
      "0.0454276981277\n",
      "0.0473343721345\n",
      "0.0490275737621\n",
      "0.0543569263478\n",
      "0.0473433866764\n",
      "0.025775057304\n",
      "0.0373428845219\n",
      "0.030780120483\n",
      "0.0420138290863\n",
      "0.0378092771534\n",
      "0.0476818880102\n",
      "0.0355694066942\n",
      "0.0399324662379\n",
      "0.0495954312665\n",
      "0.0426522602605\n",
      "0.0284777596144\n",
      "0.0454937102788\n",
      "0.0473317359581\n",
      "0.0486130853527\n",
      "0.0475989931895\n",
      "0.0509051535729\n",
      "0.0513229625254\n",
      "0.0473492068706\n",
      "0.0320462318322\n",
      "0.0426477858777\n",
      "0.0459192225968\n",
      "0.0479819055043\n",
      "0.0443778745274\n",
      "0.037305333985\n",
      "0.0453927910672\n",
      "0.0457380560134\n",
      "0.0334406817915\n",
      "0.0498349831475\n",
      "0.0723709799355\n",
      "0.0359484092023\n",
      "0.0600911668572\n",
      "0.0286093885601\n",
      "0.0498622641923\n",
      "0.0525282221016\n",
      "0.014987958253\n",
      "0.0497386356037\n",
      "0.0476614701372\n",
      "-0.00945904204197\n",
      "0.0495822037443\n",
      "0.0391855504295\n",
      "0.044476012827\n",
      "0.0451064774003\n",
      "0.0510397083051\n",
      "0.0472416959755\n",
      "0.0278093057555\n",
      "0.0426780946411\n",
      "0.0440740164848\n",
      "0.0390518589388\n",
      "0.0474812580031\n",
      "0.0405154311549\n",
      "0.0455428682761\n",
      "0.0334196441143\n",
      "0.0405978783399\n",
      "0.0516224967363\n",
      "0.044116983325\n",
      "0.0556162118396\n",
      "0.0371920755388\n",
      "0.0221598956508\n",
      "0.0390364007094\n",
      "0.0484411146939\n",
      "0.0458994051792\n",
      "0.0736605848061\n",
      "0.105400110768\n",
      "0.0485093444237\n",
      "0.044863778837\n",
      "0.0424993528421\n",
      "0.0465012359928\n",
      "0.0457494584735\n",
      "0.0575512116705\n",
      "0.0478445927876\n",
      "0.0247863983452\n",
      "0.0348841664011\n",
      "0.0370684140215\n",
      "0.0497311977027\n",
      "0.0518842718568\n",
      "0.0474421954903\n",
      "0.0451181050101\n",
      "0.0377194690636\n",
      "0.0450340471348\n",
      "0.0354933327593\n",
      "0.0350684575523\n",
      "0.0493114005541\n",
      "0.0390689383625\n",
      "0.0459823494173\n",
      "0.0509093938256\n",
      "0.0521724760596\n",
      "0.0292545203634\n",
      "0.0418492904124\n",
      "0.04760113245\n",
      "0.0487531162126\n",
      "0.0471251925012\n",
      "0.0517459393737\n",
      "0.053283900796\n",
      "0.0349046362225\n",
      "0.0509340335798\n",
      "0.0403320693944\n",
      "0.0405982626272\n",
      "0.0601975999605\n",
      "0.0554934030987\n",
      "0.0505459946359\n",
      "0.0460473189378\n",
      "0.0524653718599\n",
      "0.0473105571348\n",
      "0.0490065606086\n",
      "0.0394813188261\n",
      "0.0478095537776\n",
      "0.0413802880102\n",
      "0.0467999078814\n",
      "0.0427194689991\n",
      "0.0473630440059\n",
      "0.0440340635478\n",
      "0.0461183427256\n",
      "0.0471034224973\n",
      "0.0447415070589\n",
      "0.038267804421\n",
      "0.0476058954117\n",
      "-0.0247284726305\n",
      "0.0369072959547\n",
      "0.0493536180565\n",
      "0.0549644025908\n",
      "0.0373667624044\n",
      "0.0393105602558\n",
      "0.0586085761904\n",
      "0.0483743769272\n",
      "0.0482027350778\n",
      "0.0571625134782\n",
      "0.0574735517802\n",
      "0.0417333511979\n",
      "0.0384244809786\n",
      "0.0464461076833\n",
      "0.0420602389465\n",
      "0.0442117817013\n",
      "0.0540937792776\n",
      "0.0493303641603\n",
      "0.0477684843557\n",
      "0.0425452026919\n",
      "0.00905095725042\n",
      "0.0480077957651\n",
      "0.0498649499235\n",
      "0.038598987182\n",
      "0.042439361602\n",
      "0.0423525985531\n",
      "0.0480072329446\n",
      "0.04287414381\n",
      "0.0279133413634\n",
      "0.0447490765097\n",
      "0.0434476836551\n",
      "0.0566922738249\n",
      "0.0517182403538\n",
      "0.0495757999759\n",
      "0.0505817652041\n",
      "0.0447000148838\n",
      "0.0357805590889\n",
      "0.0343290959153\n",
      "0.052608364443\n",
      "0.0464665998852\n",
      "0.0418335594618\n",
      "0.0337824991311\n",
      "0.0437553628\n",
      "0.0437638861516\n",
      "0.0483633630944\n",
      "0.046342917997\n",
      "0.0397326759508\n",
      "0.0465717596803\n",
      "-0.052317497755\n",
      "0.0411996220459\n",
      "0.0491403467206\n",
      "0.0473788673493\n",
      "0.0374960082752\n",
      "0.045948770759\n",
      "0.0465114509063\n",
      "0.0479571763895\n",
      "0.0511311456808\n",
      "0.0438247183847\n",
      "0.0521120737368\n",
      "0.048021316788\n",
      "0.0571949347225\n",
      "0.0193364610887\n",
      "0.051536559271\n",
      "0.047472735103\n",
      "0.0486545458265\n",
      "0.0518063498385\n",
      "0.043953555242\n",
      "0.0506563160884\n",
      "0.0408691454186\n",
      "0.0497730341362\n",
      "0.0475885328977\n",
      "0.0473354904104\n",
      "0.0488984299875\n",
      "0.0374678849912\n",
      "0.0493410282432\n",
      "0.0467306153871\n",
      "0.0479706234573\n",
      "0.0487565756287\n",
      "0.0404868874791\n",
      "0.0491500604182\n",
      "0.0566419343425\n",
      "0.0486499096509\n",
      "0.0448891993277\n",
      "0.0435290308615\n",
      "0.0468895532351\n",
      "0.0072560823309\n",
      "0.0499765634193\n",
      "0.0414840005836\n",
      "0.0506975160685\n",
      "0.041578507891\n",
      "0.0363897929882\n",
      "0.048476412914\n",
      "0.0497317819673\n",
      "0.0471336067661\n",
      "0.0459268003294\n",
      "0.0376943114479\n",
      "0.0415436839112\n",
      "0.0474640595898\n",
      "0.050235105956\n",
      "0.0457575271933\n",
      "0.0477084860754\n",
      "0.0500546857622\n",
      "0.0491605901238\n",
      "0.046107862503\n",
      "0.0465285119902\n",
      "0.0438966165524\n",
      "0.0479938266623\n",
      "0.0461757162828\n",
      "0.044617508691\n",
      "0.0469711159111\n",
      "0.0497047277139\n",
      "0.0417935269103\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sents)):\n",
    "    result = 1 - spatial.distance.cosine(features[i], query_vector)\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
